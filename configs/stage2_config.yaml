# Stage 2: Cross-Modal Mapping
# Learn visual delta to audio control mapping

defaults:
  - default

stage: 2
stage_name: "cross_modal_mapping"

# Data for Stage 2
data:
  # Cross-modal dataset (2-A, 2-B)
  cross_modal:
    dataset_path: "${project.data_dir}/cross_modal_triplets"
    num_samples: 20000
    batch_size: 8  # Smaller due to generation

  # Subjectivity dataset (2-C)
  subjectivity:
    dataset_path: "${project.data_dir}/subjectivity_space"
    num_samples: 3000
    batch_size: 4
    min_validity: 0.6  # Minimum validity score for candidates

# Load pretrained components
pretrained:
  prior_estimator: "${project.checkpoint_dir}/stage0/prior_estimator.pt"
  s_proxy_stats: "${project.checkpoint_dir}/stage1/S_proxy_statistics.pt"
  audio_generator: "${project.checkpoint_dir}/stage1/generator_best.pt"

# Training phases
training:
  # Phase 2-A: g only (no generation)
  phase_2a:
    epochs: 100
    learning_rate: 1.0e-4
    freeze_generator: true
    num_samples_per_input: 1
    loss_weights:
      pseudo: 0.3
      manifold: 0.4
      identity: 0.2
      mono: 0.1

  # Phase 2-B: end-to-end with generation
  phase_2b:
    epochs: 150
    learning_rate: 5.0e-5
    freeze_generator: false
    use_lora: true
    num_samples_per_input: 4  # K=4 samples
    loss_weights:
      preserve: 0.3
      coherence: 0.4
      consistency: 0.2
      rank: 0.1

  # Phase 2-C: subjectivity space learning
  phase_2c:
    epochs: 50
    learning_rate: 1.0e-4
    freeze_s_encoder: true
    freeze_prior_estimator: true
    num_valid_candidates: 5  # Sample multiple valid interpretations
    loss_weights:
      direction: 0.5
      manifold: 0.2
      bounded: 0.2
      prior: 0.1

# Model components
model:
  # Visual Delta Encoder
  visual_encoder:
    low_level:
      backbone: "resnet18"
      output_dim: 256
    high_level:
      backbone: "clip"
      model_name: "ViT-L/14"
      output_dim: 512

  # Delta Mapping Module
  delta_mapping:
    delta_dim: 512
    num_heads: 6
    head_dim: 64
    hidden_dim: 256
    inertia_epsilon: 1.0e-3

  # P_align (Manifold Projection)
  p_align:
    per_head: true
    use_statistics: true  # Use S_proxy statistics

  # Î´C Predictor (Stage 2-C)
  delta_c_predictor:
    visual_dim: 512  # CLIP embedding
    audio_dim: 512   # CLAP embedding
    hidden_dim: 512
    output_dim: 1536  # N_v * 6 = 256 * 6
    N_v: 256
    num_heads: 6

# Losses
losses:
  # Phase 2-A specific
  pseudo:
    weight: 0.3
    use_huber: true
    huber_delta: 1.0

  manifold:
    weight: 0.4
    use_statistics: true
    moment_matching: true

  identity:
    weight: 0.2
    test_ratio: 0.1  # 10% of batch

  mono:
    weight: 0.1
    margin: 0.0

  # Phase 2-B specific
  preserve:
    weight: 0.3
    beta: 1.0
    heads: ["rhythm", "harmony", "energy"]

  coherence:
    weight: 0.4
    high_level_weight: 1.0
    low_level_weight: 0.5
    kappa: 0.0  # margin
    eta_r: 0.1  # active threshold

  consistency:
    weight: 0.2
    struct_heads: ["rhythm", "harmony", "energy"]
    style_heads: ["timbre", "space", "texture"]
    nu_h: 0.05  # minimum diversity
    max_struct_var: 0.1

  rank:
    weight: 0.1
    type: "pairwise"

  # Phase 2-C specific
  direction:
    weight: 0.5
    use_s_encoder: true

  bounded:
    weight: 0.2
    max_struct_var: 0.1
    min_style_var: 0.05

  prior_reg:
    weight: 0.1
    epsilon_prior: 0.5

# Hard Prior Rules (for coherence loss)
hard_prior_rules:
  use_rules: true
  rules_file: "${project.checkpoint_dir}/stage0/rules.yaml"

# Output
output:
  save_visual_encoder: true
  save_delta_mapping: true
  save_generator_lora: true
  output_path: "${project.checkpoint_dir}/stage2"

# Validation
validation:
  val_split: 0.1
  val_every_n_epochs: 5
  metrics:
    min_preserve_rhythm: 0.85
    min_preserve_harmony: 0.85
    min_coherence: 0.5
    min_consistency: 0.8
    min_rank_spearman: 0.7

# Inference settings
inference:
  noise_level: 0.5  # Default diffusion noise (0.3-0.7)
  num_inference_steps: 50
  guidance_scale: 7.5
