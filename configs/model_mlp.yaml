# DeltaV2A MLP Training Configuration (Phase A-C)

device: "cuda"

# === Data ===
data:
  # Source (original, non-augmented) data used for training/inference
  audio_dir: "data/original/audio"
  # Optional explicit list (one path per line). If set, this takes precedence over audio_dir.
  audio_list: null
  # Optional split manifest from scripts/build_audio_splits.py (JSONL).
  # If set, this takes precedence over audio_list/audio_dir.
  audio_split_manifest: "data/original/splits/manifest.jsonl"
  audio_split: "train" # one of: train | val | test
  image_dir: "data/original/images"
  max_audio_files: null # null = use all
  max_image_files: null # null = use all
  # Augmented artifacts produced during inverse-mapping DB build
  save_augmented_audio: true
  augmented_audio_dir: "data/augmented/pipeline/audio"

# === Models ===
model:
  clip:
    name: "ViT-L-14"
    pretrained: "openai"
  clap:
    model_id: 1
    enable_fusion: false
    max_duration: 20.0
    sample_rate: 48000

# === Phase A-1: Vocabulary ===
vocab:
  img_terms: null # null = use default IMG_VOCAB_TERMS
  aud_terms: null # null = use default AUD_VOCAB_TERMS
  # Standard 8-template ensemble for both modalities.
  # Transformation-style templates ("an image that appears more {word}") were tested
  # in attempt13 discriminativeness test and rejected — they amplify generic-transformation
  # anchor bias (warped dominated 5/7 effects). Standard templates give better coverage.
  img_prompt_templates:
    - "a {word} image"
    - "a {word} photograph"
    - "an image with a {word} mood"
    - "a {word} visual style"
    - "a {word} scene"
    - "a {word} picture"
    - "a photo that feels {word}"
    - "artwork with a {word} atmosphere"
  aud_prompt_templates:
    - "a {word} sound"
    - "a {word} music track"
    - "audio with a {word} mood"
    - "a {word} sonic texture"
    - "a {word} musical atmosphere"
    - "a {word} sounding track"
    - "music that feels {word}"
    - "a {word} audio style"

# === Phase A-2: Inverse Mapping Database (Music / Controller) ===
inverse_mapping:
  use_delta_clap: true # true: style label from CLAP(A')-CLAP(A), false: from CLAP(A')
  augmentations_per_audio: 60
  temperature: 0.1
  seed: 42
  min_active_effects: 1
  max_active_effects: 2
  # Minimum intensity for active-effect parameters in normalized [0,1] space.
  # Ensures effects are audibly strong enough to produce meaningful CLAP deltas.
  # - Params with bypass at 0 (e.g. reverb wet_level, delay mix): sampled from [min, 1]
  # - Params with bypass at 1 (e.g. bitcrush bit_depth, lowpass cutoff): sampled from [0, 1-min]
  param_min_intensity: 0.35
  # For middle-bypass params (e.g. playback_rate around 1.0x), exclude a dead-zone
  # around bypass to avoid weak/no-op samples.
  param_mid_bypass_exclusion: 0.15
  # Delta-CLAP SNR controls:
  # - reject low-norm deltas and resample up to delta_resample_attempts
  # - if all attempts fail, keep the highest-norm candidate (preserves record count)
  delta_min_norm: 0.06
  delta_resample_attempts: 3
  # Force a higher share of single-effect samples when range includes 1.
  # 0.0 = no bias, 1.0 = always single-effect.
  single_effect_ratio: 0.0
  # Optional label-quality filters for style soft labels.
  # entropy uses bits; set null to disable.
  label_entropy_max_bits: null
  label_top1_min_mass: 0.0
  label_min_margin: 0.0
  label_resample_attempts: 1
  # Weighted effect sampling for active-effect selection in Phase A-3.
  # Higher value -> selected more frequently.
  effect_sampling_weights:
    lowpass: 1.0
    bitcrush: 1.0
    reverb: 1.0
    highpass: 1.0
    distortion: 1.0
    playback_rate: 1.0
    delay: 1.0

# === Phase A-3: Inverse Mapping Database (Image / Siamese) ===
image_inverse_mapping:
  augmentations_per_image: 2
  effect_types:
    [
      "adaptive_blur",
      "motion_blur",
      "adaptive_sharpen",
      "add_noise",
      "spread",
      "sepia_tone",
      "solarize",
    ]
  intensity_min: 0.1
  intensity_max: 1.0
  seed: 42
  save_augmented_images: false
  augmented_image_dir: "data/augmented/pipeline/images"

# === Pedalboard Effects ===
effects:
  active:
    - lowpass
    - bitcrush
    - reverb
    - highpass
    - distortion
    - playback_rate
    - delay

# === Phase B: Controller Training ===
controller:
  # Style-only input: CLAP embedding adds near-zero info (probe shows clap_only ≈ mean baseline).
  # Setting audio_embed_dim=0 removes 512-dim noise and forces the model to rely on
  # the 24-dim delta-CLAP style label which does carry useful signal.
  audio_embed_dim: 512
  fusion_mode: "gated_residual" # one of: concat, gated_residual
  audio_gate_bias: -2.0
  hidden_dims: [256, 256, 128]
  dropout: 0.3
  weight_decay: 0.001
  lr_scheduler: "cosine"
  lr_min: 1.0e-6
  confidence_weighting_enabled: false
  confidence_weight_power: 1.0
  confidence_min_weight: 0.2
  confidence_use_delta_norm: false
  confidence_style_alpha: 1.0
  confidence_delta_scale: 1.0
  use_activity_head: true
  activity_loss_weight: 0.6  # attempt8=0.5 -> 0.6: slight boost for F1, param_loss stays primary
  activity_mismatch_weight: 2.0
  activity_mismatch_gamma: 2.0
  activity_loss_type: "asl" # one of: bce, focal, asl
  focal_gamma: 2.0
  asl_gamma_pos: 0.0
  asl_gamma_neg: 5.0  # attempt8=4.0 -> 5.0: moderate hard-negative boost (6.0 in attempt9 hurt param head)
  asl_clip: 0.05
  param_loss_weight: 1.0
  fp_param_weight: 0.0    # removed: 0.5 in attempt9 biased param head toward bypass, degraded active_rmse
  inactive_param_weight: 0.0
  param_loss_type: "huber" # one of: mse, huber
  huber_delta: 0.02
  selection_metric: "val_active_param_rmse_gated"
  train_backbone: true
  train_param_head: true
  train_activity_head: true
  balanced_sampler:
    enabled: true
    mode: "inverse_frequency" # one of: count_boost, inverse_frequency
    effects: ["lowpass", "bitcrush", "reverb", "highpass", "distortion", "playback_rate", "delay"]
    boost: 2.0
    base_weight: 1.0
  effect_loss_weights:
    lowpass: 2.0     # 1.0 -> 2.0 (F1=0.44 at 0.5 threshold, active_rmse=0.30)
    bitcrush: 2.5    # 1.5 -> 2.5 (worst: F1=0.40, active_rmse=0.28-0.34)
    reverb: 1.5      # 1.0 -> 1.5 (damping active_rmse=0.26)
    highpass: 2.0    # 1.0 -> 2.0 (F1=0.44, active_rmse=0.26)
    distortion: 1.0
    playback_rate: 1.5
    delay: 3.0       # 2.0 -> 3.0 (3 hard params: mix/feedback/delay_seconds all ~0.22-0.27)
  batch_size: 256
  num_epochs: 600  # attempt13=400 -> 600 (best_epoch was 386/400, still converging at end; vocab v3)
  learning_rate: 0.0001
  val_split: 0.2
  # Single-stage joint training (attempt11+):
  # attempt13: best_epoch=386/400 with vocab v3. Style F1 improved (+0.066) but
  # controller RMSE regressed (0.1602->0.1783) due to incomplete convergence.
  # attempt14: extend to 600 epochs to allow full convergence. Reuse attempt13 DB.
  training_stages:
    - name: "joint_single_stage"
      epochs: 600
      learning_rate: 0.0001
      lr_scheduler_type: "cosine"
      lr_min: 1.0e-6
      activity_loss_weight: 0.6
      activity_mismatch_weight: 2.0
      activity_mismatch_gamma: 2.0
      activity_loss_type: "asl"
      asl_gamma_pos: 0.0
      asl_gamma_neg: 5.0
      asl_clip: 0.05
      param_loss_weight: 1.0
      train_backbone: true
      train_param_head: true
      train_activity_head: true
      selection_metric: "val_active_param_rmse_gated"
      threshold_tuning:
        # balanced: constrain F1 >= 95% of default-0.5-threshold F1
        # prevents degenerate threshold=0.05 (always-active) solution
        objective: "active_param_rmse_gated_balanced"
        coord_passes: 2
        num_thresholds: 37
        threshold_min: 0.05
        threshold_max: 0.95
        min_macro_f1_ratio: 0.95
        min_micro_f1_ratio: 0.95
        f1_penalty_weight: 1.0
  post_train_analysis:
    enabled: true
    out_dir: null # null -> outputs/pipeline/controller/analysis
    val_split: null # null -> reuse controller.val_split
    split_seed: 42
    batch_size: 128
    num_renders: 4
    render_selection: "best_worst" # one of: random, top_mae, best_worst
    render_seed: 42
    sample_rate: null # null -> reuse model.clap.sample_rate
    max_duration: null # null -> reuse model.clap.max_duration
    device: null # null -> reuse global device
    manifest_path: null # null -> data.augmented_audio_dir/manifest.jsonl
    hidden_dims: null # null -> reuse controller.hidden_dims
    dropout: null # null -> reuse controller.dropout

# === Phase B-AR: AR Hybrid Controller Training ===
ar_controller:
  # AR model excluded from attempt9 onwards: MLP decisively outperforms AR
  # (MLP active_only_rmse_gated ~0.237 vs AR ~0.386). Run train_ar step is skipped.
  condition_dim: 256
  hidden_dim: 512
  dropout: 0.1
  max_steps: 2
  clap_embed_dim: 512
  num_epochs: 150
  batch_size: 256
  learning_rate: 0.0001
  weight_decay: 0.001
  val_split: 0.2
  seed: 42
  effect_loss_weight: 1.0
  param_loss_weight: 50.0
  huber_delta: 0.02
  auto_effect_class_weights: true
  effect_class_weight_power: 0.5
  effect_class_weight_min: 0.5
  effect_class_weight_max: 3.0
  effect_ce_label_smoothing: 0.03
  confidence_weighting_enabled: false
  confidence_weight_power: 1.0
  confidence_min_weight: 0.2
  confidence_use_delta_norm: false
  confidence_style_alpha: 1.0
  confidence_delta_scale: 1.0
  param_effect_weights:
    lowpass: 1.0
    bitcrush: 1.3
    reverb: 1.8
    highpass: 1.0
    distortion: 1.0
    playback_rate: 1.5
    delay: 2.0
  lr_scheduler: "cosine"
  lr_min: 1.0e-6
  post_train_analysis:
    enabled: false  # AR excluded from attempt9+
    out_dir: null
    val_split: null
    split_seed: 42
    num_renders: 4
    render_selection: "best_worst"
    render_seed: 42
    sample_rate: null
    max_duration: null
    device: null
    manifest_path: null

# === Phase C: Visual Encoder ===
visual_encoder:
  # true: train/use Siamese visual encoder
  # false: skip Siamese and use direct CLIP similarity delta Sim(I')-Sim(I)
  enabled: false
  projection_dim: 768
  dropout: 0.1
  training:
    enabled: false
    batch_size: 32
    num_epochs: 40
    learning_rate: 0.0001
    val_split: 0.2
    seed: 42

# === Phase C: Inference ===
inference:
  top_k: 5

# === Output ===
output:
  dir: "outputs/pipeline_mlp"
