# DeltaV2A Pipeline Configuration (Phase A-C)

device: "mps"

# === Data ===
data:
  # Source (original, non-augmented) data used for training/inference
  audio_dir: "data/original/audio"
  # Optional explicit list (one path per line). If set, this takes precedence over audio_dir.
  audio_list: null
  # Optional split manifest from scripts/build_audio_splits.py (JSONL).
  # If set, this takes precedence over audio_list/audio_dir.
  audio_split_manifest: "data/original/splits/manifest.jsonl"
  audio_split: "train"  # one of: train | val | test
  image_dir: "data/original/images"
  max_audio_files: null  # null = use all
  max_image_files: null  # null = use all
  # Augmented artifacts produced during inverse-mapping DB build
  save_augmented_audio: true
  augmented_audio_dir: "data/augmented/pipeline/audio"

# === Models ===
model:
  clip:
    name: "ViT-L-14"
    pretrained: "openai"
  clap:
    model_id: 1
    enable_fusion: false
    max_duration: 20.0
    sample_rate: 48000

# === Phase A-1: Vocabulary ===
vocab:
  img_terms: null  # null = use default IMG_VOCAB_TERMS
  aud_terms: null  # null = use default AUD_VOCAB_TERMS

# === Phase A-2: Correspondence ===
correspondence:
  sbert_model: "all-MiniLM-L6-v2"

# === Phase A-3: Inverse Mapping Database ===
inverse_mapping:
  augmentations_per_audio: 10
  temperature: 0.1
  seed: 42
  min_active_effects: 1
  max_active_effects: 2
  # Weighted effect sampling for active-effect selection in Phase A-3.
  # Higher value -> selected more frequently.
  effect_sampling_weights:
    lowpass: 1.0
    bitcrush: 2.0
    reverb: 1.0
    highpass: 1.0
    distortion: 1.0
    phaser: 2.5
    delay: 1.0

# === Pedalboard Effects ===
effects:
  active:
    - lowpass
    - bitcrush
    - reverb
    - highpass
    - distortion
    - phaser
    - delay

# === Phase B: Controller Training ===
controller:
  hidden_dims: [512, 256, 128]
  dropout: 0.1
  batch_size: 64
  num_epochs: 100
  learning_rate: 0.0001
  val_split: 0.2
  post_train_analysis:
    enabled: true
    out_dir: null  # null -> outputs/pipeline/controller/analysis
    val_split: null  # null -> reuse controller.val_split
    split_seed: 42
    batch_size: 128
    num_renders: 5
    sample_rate: null  # null -> reuse model.clap.sample_rate
    max_duration: null  # null -> reuse model.clap.max_duration
    device: null  # null -> reuse global device
    manifest_path: null  # null -> data.augmented_audio_dir/manifest.jsonl
    hidden_dims: null  # null -> reuse controller.hidden_dims
    dropout: null  # null -> reuse controller.dropout

# === Phase C: Visual Encoder ===
visual_encoder:
  projection_dim: 768
  dropout: 0.1
  training:
    enabled: true
    batch_size: 32
    num_epochs: 40
    learning_rate: 0.0001
    val_split: 0.2
    augmentations_per_image: 2
    effect_types: ["adaptive_blur", "motion_blur", "adaptive_sharpen", "add_noise", "spread", "sepia_tone", "solarize"]
    intensities: ["low", "mid", "high"]
    style_temperature: 0.07
    contrastive_margin: 0.3
    loss_weight_align: 1.0
    loss_weight_style: 0.5
    loss_weight_contrastive: 0.2
    seed: 42
    save_augmented_images: true
    augmented_image_dir: "data/augmented/pipeline/images"

# === Phase C: Inference ===
inference:
  top_k: 5

# === Output ===
output:
  dir: "outputs/pipeline"
