# DeltaV2A Pipeline Configuration (Phase A-C)

device: "mps"

# === Data ===
data:
  # Source (original, non-augmented) data used for training/inference
  audio_dir: "data/original/audio"
  # Optional explicit list (one path per line). If set, this takes precedence over audio_dir.
  audio_list: null
  # Optional split manifest from scripts/build_audio_splits.py (JSONL).
  # If set, this takes precedence over audio_list/audio_dir.
  audio_split_manifest: "data/original/splits/manifest.jsonl"
  audio_split: "train" # one of: train | val | test
  image_dir: "data/original/images"
  max_audio_files: null # null = use all
  max_image_files: null # null = use all
  # Augmented artifacts produced during inverse-mapping DB build
  save_augmented_audio: true
  augmented_audio_dir: "data/augmented/pipeline/audio"

# === Models ===
model:
  clip:
    name: "ViT-L-14"
    pretrained: "openai"
  clap:
    model_id: 1
    enable_fusion: false
    max_duration: 20.0
    sample_rate: 48000

# === Phase A-1: Vocabulary ===
vocab:
  img_terms: null # null = use default IMG_VOCAB_TERMS
  aud_terms: null # null = use default AUD_VOCAB_TERMS

# === Phase A-2: Inverse Mapping Database (Music / Controller) ===
inverse_mapping:
  augmentations_per_audio: 30
  temperature: 0.1
  seed: 42
  min_active_effects: 1
  max_active_effects: 3
  # Weighted effect sampling for active-effect selection in Phase A-3.
  # Higher value -> selected more frequently.
  effect_sampling_weights:
    lowpass: 1.0
    bitcrush: 1.0
    reverb: 1.0
    highpass: 1.0
    distortion: 1.0
    playback_rate: 1.0
    delay: 1.0

# === Phase A-3: Inverse Mapping Database (Image / Siamese) ===
image_inverse_mapping:
  augmentations_per_image: 2
  effect_types:
    [
      "adaptive_blur",
      "motion_blur",
      "adaptive_sharpen",
      "add_noise",
      "spread",
      "sepia_tone",
      "solarize",
    ]
  intensity_min: 0.1
  intensity_max: 1.0
  seed: 42
  save_augmented_images: false
  augmented_image_dir: "data/augmented/pipeline/images"

# === Pedalboard Effects ===
effects:
  active:
    - lowpass
    - bitcrush
    - reverb
    - highpass
    - distortion
    - playback_rate
    - delay

# === Phase B: Controller Training ===
controller:
  hidden_dims: [1024, 512, 512, 256]
  dropout: 0.2
  use_activity_head: true
  activity_loss_weight: 0.8
  activity_mismatch_weight: 2.0
  activity_mismatch_gamma: 2.0
  activity_loss_type: "asl" # one of: bce, focal, asl
  focal_gamma: 2.0
  asl_gamma_pos: 0.0
  asl_gamma_neg: 4.0
  asl_clip: 0.05
  param_loss_weight: 1.0
  inactive_param_weight: 0.0
  param_loss_type: "huber" # one of: mse, huber
  huber_delta: 0.02
  selection_metric: "val_activity_macro_f1" # one of: val_loss, val_param_loss, val_active_param_rmse, val_activity_macro_f1, val_activity_micro_f1
  train_backbone: true
  train_param_head: true
  train_activity_head: true
  balanced_sampler:
    enabled: true
    mode: "inverse_frequency" # one of: count_boost, inverse_frequency
    effects: ["lowpass", "bitcrush", "reverb", "highpass", "distortion", "playback_rate", "delay"]
    boost: 2.0
    base_weight: 1.0
  effect_loss_weights:
    lowpass: 1.0
    bitcrush: 1.5
    reverb: 1.0
    highpass: 1.0
    distortion: 1.0
    playback_rate: 2.5
    delay: 4.0
  batch_size: 256
  num_epochs: 100
  learning_rate: 0.0001
  val_split: 0.2
  training_stages:
    - name: "stage1_activity_warmup"
      epochs: 15
      learning_rate: 0.0001
      activity_loss_weight: 1.0
      activity_mismatch_weight: 3.0
      activity_mismatch_gamma: 2.0
      activity_loss_type: "asl"
      param_loss_weight: 0.0
      train_backbone: true
      train_param_head: false
      train_activity_head: true
      selection_metric: "val_activity_macro_f1"
    - name: "stage2_joint_main"
      epochs: 85
      learning_rate: 0.00008
      activity_loss_weight: 0.7
      activity_mismatch_weight: 2.0
      activity_mismatch_gamma: 2.0
      activity_loss_type: "asl"
      param_loss_weight: 1.0
      train_backbone: true
      train_param_head: true
      train_activity_head: true
      selection_metric: "val_activity_macro_f1"
  post_train_analysis:
    enabled: true
    out_dir: null # null -> outputs/pipeline/controller/analysis
    val_split: null # null -> reuse controller.val_split
    split_seed: 42
    batch_size: 128
    num_renders: 5
    sample_rate: null # null -> reuse model.clap.sample_rate
    max_duration: null # null -> reuse model.clap.max_duration
    device: null # null -> reuse global device
    manifest_path: null # null -> data.augmented_audio_dir/manifest.jsonl
    hidden_dims: null # null -> reuse controller.hidden_dims
    dropout: null # null -> reuse controller.dropout

# === Phase C: Visual Encoder ===
visual_encoder:
  # true: train/use Siamese visual encoder
  # false: skip Siamese and use direct CLIP similarity delta Sim(I')-Sim(I)
  enabled: false
  projection_dim: 768
  dropout: 0.1
  training:
    enabled: false
    batch_size: 32
    num_epochs: 40
    learning_rate: 0.0001
    val_split: 0.2
    seed: 42

# === Phase C: Inference ===
inference:
  top_k: 5

# === Output ===
output:
  dir: "outputs/pipeline"
